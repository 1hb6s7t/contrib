{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS14JBgcFwr7"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Type, Union, List\n",
        "# import numpy as np\n",
        "\n",
        "import mindspore as ms\n",
        "import mindspore.nn as nn\n",
        "import mindspore.dataset as ds\n",
        "import mindspore.dataset.vision.c_transforms as vision\n",
        "\n",
        "# from mindvision.engine.callback import ValAccMonitor\n",
        "from mindvision.classification.models.blocks import ConvNormActivation\n",
        "\n",
        "from mindvision.classification.models.backbones import ResidualBlockBase, ResidualBlock, ResNet\n",
        "from mindvision.classification.models.classifiers import BaseClassifier\n",
        "from mindvision.classification.models.head import DenseHead\n",
        "from mindvision.classification.models.neck import GlobalAvgPooling\n",
        "from mindvision.classification.utils.model_urls import model_urls\n",
        "from mindvision.utils.load_pretrained_model import LoadPretrainedModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet-50 定义"
      ],
      "metadata": {
        "id": "f2SwUxmZGim3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlockBase(nn.Cell):\n",
        "    expansion: int = 1  # 最后一个个卷积核数量与第一个卷积核数量相等\n",
        "\n",
        "    def __init__(self, in_channel: int, out_channel: int,\n",
        "                 stride: int = 1, norm: Optional[nn.Cell] = None,\n",
        "                 down_sample: Optional[nn.Cell] = None) -> None:\n",
        "        super(ResidualBlockBase, self).__init__()\n",
        "        if not norm:\n",
        "            norm = nn.BatchNorm2d\n",
        "\n",
        "        self.conv1 = ConvNormActivation(in_channel, out_channel,\n",
        "                                        kernel_size=3, stride=stride, norm=norm)\n",
        "        self.conv2 = ConvNormActivation(out_channel, out_channel,\n",
        "                                        kernel_size=3, norm=norm, activation=None)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down_sample = down_sample\n",
        "\n",
        "    def construct(self, x):\n",
        "        \"\"\"ResidualBlockBase construct.\"\"\"\n",
        "        identity = x  # shortcuts分支\n",
        "\n",
        "        out = self.conv1(x)  # 主分支第一层：3*3卷积层\n",
        "        out = self.conv2(out)  # 主分支第二层：3*3卷积层\n",
        "\n",
        "        if self.down_sample:\n",
        "            identity = self.down_sample(x)\n",
        "        out += identity  # 输出为主分支与shortcuts之和\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResidualBlock(nn.Cell):\n",
        "    expansion = 4  # 最后一个卷积核的数量是第一个卷积核数量的4倍\n",
        "\n",
        "    def __init__(self, in_channel: int, out_channel: int,\n",
        "                 stride: int = 1, norm: Optional[nn.Cell] = None,\n",
        "                 down_sample: Optional[nn.Cell] = None) -> None:\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        if not norm:\n",
        "            norm = nn.BatchNorm2d\n",
        "\n",
        "        self.conv1 = ConvNormActivation(in_channel, out_channel,\n",
        "                                        kernel_size=1, norm=norm)\n",
        "        self.conv2 = ConvNormActivation(out_channel, out_channel,\n",
        "                                        kernel_size=3, stride=stride, norm=norm)\n",
        "        self.conv3 = ConvNormActivation(out_channel, out_channel * self.expansion,\n",
        "                                        kernel_size=1, norm=norm, activation=None)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down_sample = down_sample\n",
        "\n",
        "    def construct(self, x):\n",
        "        identity = x  # shortscuts分支\n",
        "\n",
        "        out = self.conv1(x)  # 主分支第一层：1*1卷积层\n",
        "        out = self.conv2(out)  # 主分支第二层：3*3卷积层\n",
        "        out = self.conv3(out)  # 主分支第三层：1*1卷积层\n",
        "\n",
        "        if self.down_sample:\n",
        "            identity = self.down_sample(x)\n",
        "\n",
        "        out += identity  # 输出为主分支与shortcuts之和\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def make_layer(last_out_channel, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
        "               channel: int, block_nums: int, stride: int = 1):\n",
        "    down_sample = None  # shortcuts分支\n",
        "\n",
        "    if stride != 1 or last_out_channel != channel * block.expansion:\n",
        "        down_sample = ConvNormActivation(last_out_channel, channel * block.expansion,\n",
        "                                         kernel_size=1, stride=stride, norm=nn.BatchNorm2d, activation=None)\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(last_out_channel, channel, stride=stride, down_sample=down_sample, norm=nn.BatchNorm2d))\n",
        "\n",
        "    in_channel = channel * block.expansion\n",
        "    # 堆叠残差网络\n",
        "    for _ in range(1, block_nums):\n",
        "        layers.append(block(in_channel, channel, norm=nn.BatchNorm2d))\n",
        "\n",
        "    return nn.SequentialCell(layers)\n",
        "\n",
        "class ResNet(nn.Cell):\n",
        "    def __init__(self, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
        "                 layer_nums: List[int], norm: Optional[nn.Cell] = None) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if not norm:\n",
        "            norm = nn.BatchNorm2d\n",
        "        # 第一个卷积层，输入channel为3（彩色图像），输出channel为64\n",
        "        self.conv1 = ConvNormActivation(3, 64, kernel_size=7, stride=2, norm=norm)\n",
        "        # 最大池化层，缩小图片的尺寸\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
        "        # 各个残差网络结构块定义，\n",
        "        self.layer1 = make_layer(64, block, 64, layer_nums[0])\n",
        "        self.layer2 = make_layer(64 * block.expansion, block, 128, layer_nums[1], stride=2)\n",
        "        self.layer3 = make_layer(128 * block.expansion, block, 256, layer_nums[2], stride=2)\n",
        "        self.layer4 = make_layer(256 * block.expansion, block, 512, layer_nums[3], stride=2)\n",
        "\n",
        "    def construct(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _resnet(arch: str, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
        "            layers: List[int], num_classes: int, pretrained: bool, input_channel: int):\n",
        "    backbone = ResNet(block, layers)\n",
        "    neck = GlobalAvgPooling()  # 平均池化层\n",
        "    head = DenseHead(input_channel=input_channel, num_classes=num_classes)  # 全连接层\n",
        "    model = BaseClassifier(backbone, neck, head)  # 将backbone层、neck层和head层连接起来\n",
        "\n",
        "    if pretrained:\n",
        "        # 下载并加载预训练模型\n",
        "        LoadPretrainedModel(model, model_urls[arch]).run()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(num_classes: int = 1000, pretrained: bool = False):\n",
        "    \"ResNet50模型\"\n",
        "    return _resnet(\"resnet50\", ResidualBlock, [3, 4, 6, 3], num_classes, pretrained, 2048)"
      ],
      "metadata": {
        "id": "Q7zUx7vGF6Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义数据集"
      ],
      "metadata": {
        "id": "7FULJS5OGpDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ds.ImageFolderDataset(dataset_dir=\"data2\", shuffle=True, decode=True)\n",
        "\n",
        "dataset = dataset.map(operations=[\n",
        "    vision.Resize(224),\n",
        "    vision.CenterCrop(224),\n",
        "    vision.HWC2CHW(),\n",
        "    lambda x: (x / 255).astype(\"float32\")\n",
        "], input_columns=\"image\")\n",
        "\n",
        "dataset = dataset.batch(32, drop_remainder=True)"
      ],
      "metadata": {
        "id": "u1ucb0nLGEbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练网络"
      ],
      "metadata": {
        "id": "ECOtNxiHGr8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = dataset.get_dataset_size()\n",
        "\n",
        "# 定义ResNet50网络\n",
        "network = resnet50(pretrained=False)\n",
        "\n",
        "# 全连接层输入层的大小\n",
        "in_channel = network.head.dense.in_channels\n",
        "head = DenseHead(input_channel=in_channel, num_classes=10)\n",
        "\n",
        "# 重置全连接层\n",
        "network.head = head\n",
        "\n",
        "# 设置学习率\n",
        "num_epochs = 40\n",
        "lr = nn.cosine_decay_lr(min_lr=0.00001, max_lr=0.001, total_step=step_size * num_epochs, step_per_epoch=step_size, decay_epoch=num_epochs)\n",
        "\n",
        "# 定义优化器和损失函数\n",
        "opt = nn.Momentum(params=network.trainable_params(), learning_rate=lr, momentum=0.9)\n",
        "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "\n",
        "# 实例化模型\n",
        "model = ms.Model(network, loss, opt, metrics={\"Accuracy\": nn.Accuracy()})\n",
        "\n",
        "# 模型训练\n",
        "model.train(num_epochs, dataset)"
      ],
      "metadata": {
        "id": "5UyAE3IIGVvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
