{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from  mindspore.train import Model\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "from mindspore.train.callback import LossMonitor\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint\n",
    "import mindspore.dataset.vision.c_transforms as vision\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.common import dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "epoch_size = 10 #训练轮次\n",
    "repeat_size = 1\n",
    "num_classes = 2 #分类数目\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")  #仅用于演示程序结果，实际使用服务器GPU多轮次训练的ckpt文件\n",
    "#context.set_context(mode=context.GRAPH_MODE,device_target=\"GPU\")\n",
    "train_file_name = './datasets/convert_dataset_to_mindrecord/data_to_mindrecord/train.mindrecord'\n",
    "val_file_name = './datasets/convert_dataset_to_mindrecord/data_to_mindrecord/test/val.mindrecord'\n",
    "test_file_name = './datasets/convert_dataset_to_mindrecord/data_to_mindrecord/reasoning/test.mindrecord'\n",
    "\n",
    "#创建训练数据集\n",
    "def create_dateset(file_name, batch_size=32, repeat_size=1, status='train', num_parallel_workers=1):\n",
    "    define_data_set = ds.MindDataset(file_name, columns_list=['data', 'label'])  # 读取解析MindRecord数据文件构建数据集\n",
    "    decode_op = vision.Decode()\n",
    "    define_data_set = define_data_set.map(operations=decode_op, input_columns=[\"data\"], num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    resize_height, resize_width = 227, 227\n",
    "    rescale = 1.0/255.0 #缩放因子\n",
    "    shift = 0.0 #平移因子\n",
    "\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR) #双线性插值调整尺寸大小\n",
    "\n",
    "    rescale_op = CV.Rescale(rescale, shift)#根据所给的缩放平移因子调整图像的尺寸大小 output = image * rescale + shift\n",
    "\n",
    "    # normalize_op = CV.Normalize((122.96757279 / 255, 122.96757279 / 255, 122.96757279 / 255), (55.55022323 / 255, 55.55022323 / 255, 55.55022323 / 255))  #归一化\n",
    "\n",
    "    if status == 'train':\n",
    "        random_crop_op = CV.RandomCrop([32, 32], [4, 4, 4, 4]) #(h,w)；4个数 左上右下填充\n",
    "        random_horizontal_op = CV.RandomHorizontalFlip() #按照0.5的概率水平随机翻转\n",
    "    channel_swap_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32) #tensor转指定数据\n",
    "\n",
    "    define_data_set = define_data_set.map(operations=type_cast_op, input_columns='label', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=random_crop_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=random_horizontal_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=resize_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=rescale_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=channel_swap_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    buffer_size = 10000\n",
    "\n",
    "    define_data_set = define_data_set.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    define_data_set = define_data_set.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    define_data_set = define_data_set.repeat(repeat_size)\n",
    "\n",
    "    return define_data_set\n",
    "#创建验证数据集\n",
    "def create_dateset_val(file_name, batch_size=32, repeat_size=1, status='val', num_parallel_workers=1):\n",
    "    define_data_set = ds.MindDataset(file_name, columns_list=['data', 'label'])  # 读取解析MindRecord数据文件构建数据集\n",
    "    decode_op = vision.Decode()\n",
    "    define_data_set = define_data_set.map(operations=decode_op, input_columns=[\"data\"], num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    resize_height, resize_width = 227, 227\n",
    "    rescale = 1.0 / 255.0 #缩放因子\n",
    "    shift = 0.0 #平移因子\n",
    "\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR) #双线性插值调整尺寸大小\n",
    "\n",
    "    rescale_op = CV.Rescale(rescale, shift)  #根据所给的缩放平移因子调整图像的尺寸大小 output = image * rescale + shift\n",
    "\n",
    "    # normalize_op = CV.NormalizeCV.Normalize((122.96757279 / 255, 122.96757279 / 255, 122.96757279 / 255), (55.55022323 / 255, 55.55022323 / 255, 55.55022323 / 255))  #归一化\n",
    "\n",
    "    channel_swap_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32) #tensor转指定数据\n",
    "\n",
    "    define_data_set = define_data_set.map(operations=type_cast_op, input_columns='label', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=resize_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=rescale_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "    define_data_set = define_data_set.map(operations=channel_swap_op, input_columns='data', num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    buffer_size = 10000\n",
    "\n",
    "    define_data_set = define_data_set.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    define_data_set = define_data_set.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    define_data_set = define_data_set.repeat(repeat_size)\n",
    "\n",
    "    return define_data_set\n",
    "\n",
    "\n",
    "\n",
    "class AlexNet(nn.Cell):\n",
    "    def __init__(self, num_classes=10, channel=3):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, 11, stride=4, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(96, 256, 5, stride=1, pad_mode='same')\n",
    "        self.conv3 = nn.Conv2d(256, 384, 3, stride=1, pad_mode='same')\n",
    "        self.conv4 = nn.Conv2d(384, 384, 3, stride=1, pad_mode='same')\n",
    "        self.conv5 = nn.Conv2d(384, 256, 3, stride=1, pad_mode='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(6*6*256, 4096)\n",
    "        self.fc2 = nn.Dense(4096, 4096)\n",
    "        self.fc3 = nn.Dense(4096, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x) #卷积1\n",
    "        x = self.relu(x)  #激活\n",
    "        x = self.max_pool2d(x) #池化\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "ds_train = create_dateset(train_file_name, batch_size, repeat_size)\n",
    "ds_val = create_dateset_val(val_file_name, batch_size, repeat_size)\n",
    "ds_test = create_dateset_val(test_file_name, batch_size, repeat_size)\n",
    "\n",
    "network = AlexNet(num_classes=num_classes)\n",
    "\n",
    "net_opt = nn.Momentum(network.trainable_params(), lr, momentum)\n",
    "\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "metrics = {\"Accuracy\": nn.Accuracy()}\n",
    "model = Model(network, net_loss, net_opt, metrics=metrics)\n",
    "\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=30, keep_checkpoint_max=100)\n",
    "ckpt_callback = ModelCheckpoint(prefix='alex', directory='./ckpt', config=ckpt_config)\n",
    "\n",
    "#loss_cb = LossMonitor()\n",
    "model.train(epoch_size, ds_train, callbacks=[ckpt_callback, LossMonitor(30)], dataset_sink_mode=False)\n",
    "\n",
    "result = model.eval(ds_val)\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}