{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6cb9dd",
   "metadata": {},
   "source": [
    "# MindSpore——基于多模型的狗品种分类实现\n",
    "\n",
    "本文使用[MindSpore Vision](https://mindspore.cn/vision/docs/zh-CN/r0.1/index.html)中的多种网络模型来开发一个AI应用（狗的品种分类），通过比较不同模型的分类验证精度并将训练效果最佳的网络模型部署到Android手机上，执行推理和部署功能。\n",
    "\n",
    "## 训练模型\n",
    "\n",
    "### .Mobilenetv2 、resnet50、resnet101\n",
    "\n",
    "## 数据准备与加载\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "首先需要下载本案例所用到的[狗的品种分类数据集](https://cloud.tsinghua.edu.cn/f/80013ef29c5f42728fc8/?dl=1)，该数据集中狗的种类有130种,图片数量为70482。\n",
    "\n",
    "![狗的分类识别数据集图.png](./image/狗的分类识别数据集图.png)\n",
    "\n",
    "数据集的目录结构如下所示：\n",
    "\n",
    "```text\n",
    "datasets\n",
    "└── low-resolution\n",
    "    ├── Airedale\n",
    "    │   ├── n10001.jpeg\n",
    "    │   └── n10002.jpeg\n",
    "    │   └── ...\n",
    "    │    ...\n",
    "    ├── basenji\n",
    "    │   ├── n12701.jpg\n",
    "    │   └── n12702.jpg\n",
    "    │   └── ...\n",
    "    │    ...\n",
    "```\n",
    "\n",
    "## 数据集预处理\n",
    "\n",
    "通过上面的链接，在下载数据集后，为了使模型产生更好的效果，这里我们对数据集进行了预处理，主要包括：图片聚焦、图片裁剪以及数据集的划分。\n",
    "\n",
    "### 一、图片聚焦\n",
    "\n",
    "![yolo.png](./image/yolo.png)\n",
    "\n",
    "#### 技术实现：调用YOLOv5接口(基于以前项目中部署的YOLO(torch)，只用于做数据预处理，后续各种模型训练与测试均为mindspore实现)\n",
    "\n",
    "[src文件下载以及具体环境配置参考](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "#### 采用yolov5s.pt模型，其识别类型为80种。经过YOLO接口处理后，我们就可以得到原数据集图片被识别为各对象的位置坐标。\n",
    "\n",
    "#### 下方为YOLO接口调用的代码。此过程是在本机上完成，所以用到的是绝对路径。\n",
    "\n",
    "#### 如有需要可以根据上方链接下载数据集切换路径就可以在你的本机上完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64294103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import src.detect as detect\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "images_path = 'D:\\\\download\\\\low-resolution\\\\low-resolution'#原数据集path\n",
    "path_list = os.listdir(images_path)\n",
    "for i in range(len(path_list)):\n",
    "    images_path1 = 'D:\\\\download\\\\low-resolution\\\\low-resolution' + \"\\\\\" + path_list[i]\n",
    "    detect_api = detect.DetectAPI(exist_ok=True, source=images_path1, weights='yolov5s.pt', name='exp_dog_' + i.__str__())\n",
    "    label = detect_api.run()#接口调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d7546",
   "metadata": {},
   "source": [
    "### 二、图片裁剪\n",
    "\n",
    "#### 技术实现：由上一步我们就得到了原数据集图片被识别为各个对象的位置坐标，如下表所示：\n",
    "\n",
    "| id | x | y | w | h | confidence |\n",
    "| :----:| :----: | :----: | :----:| :----: | :----: |\n",
    "| 16 | 0.522917 | 0.606944 | 0.320833 | 0.719444 | 0.870236 |\n",
    "\n",
    "id:识别图片的类别\n",
    "\n",
    "x:识别框中心坐标中横坐标占图片长度的比例\n",
    "\n",
    "y:识别框中心坐标中纵坐标坐标占图片长度的比例\n",
    "\n",
    "w:识别框长度占图片长度的比例\n",
    "\n",
    "h:识别框宽度占图片长度的比例\n",
    "\n",
    "confidence:识别对象置信度\n",
    "\n",
    "利用以上坐标通过下方的代码进行裁剪来获得识别对象的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e5798",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id_set = set() # 存放识别的类别id\n",
    "path_label = \"./runs/detect/exp_dog/labels\" # yolo识别图像后的标签\n",
    "path_image = images_path # 原图像\n",
    "path_list_label = os.listdir(path_label)\n",
    "path_list_image = os.listdir(path_image)\n",
    "#path_list_label.sort(key=lambda x:int(x.split('.')[1]))\n",
    "#path_list_image.sort(key=lambda x:int(x.split('.')[1]))\n",
    "for i in range(len(path_list_label)):\n",
    "    path1 = path_image + \"\\\\\" + path_list_image[i]\n",
    "    path2 = path_label + \"\\\\\" + path_list_label[i]\n",
    "    img = cv2.imread(path1)\n",
    "    box = Path(path2).read_text()\n",
    "    box = box.split(\"\\n\")\n",
    "    for j in range(len(box)-1):\n",
    "        dh, dw, _ = img.shape\n",
    "        class_id, x_center, y_center, w, h, conf = box[j].strip().split()\n",
    "        id_set.add(int(class_id))\n",
    "        x_center, y_center, w, h = float(x_center), float(y_center), float(w), float(h)\n",
    "        x_center = round(x_center * dw)\n",
    "        y_center = round(y_center * dh)\n",
    "        w = round(w * dw)\n",
    "        h = round(h * dh)\n",
    "        x = round(x_center - w / 2)\n",
    "        y = round(y_center - h / 2)\n",
    "        imgCrop = img[y:y + h, x:x + w]\n",
    "        conf = float(conf)\n",
    "        path3 = \"data/class_dog/\" + class_id + \"/\" + j.__str__() + \".\" + path_list_image[i]  # 保存类别图像的位置\n",
    "        cv2.imwrite(path3, imgCrop)\n",
    "        path4 = \"data/class_label_dog/\" + class_id + \"/\" + j.__str__() + \".\" + path_list_image[i]  # 保存类别图像位于原图片坐标的位置\n",
    "        file = open(path4 + \".txt\", \"w\")\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        file.write(x.__str__() + \",\" + y.__str__() + \",\" + (x + w).__str__() + \",\" + (h + y).__str__())\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdce42f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 三、数据集的划分\n",
    "\n",
    "在将数据集中的狗裁剪出来后，按照原数据集的目录结构，将图片重新组成数据集。现在的数据集目录结构与代码中的数据集目录结构不相同，对数据集进行处理，将其转换为代码中的数据集目录结构。\n",
    "\n",
    "处理后的数据集的目录结构如下所示（将每一类按照train：val：infer = 8：1：1划分）：\n",
    "\n",
    "```text\n",
    "datasets\n",
    "├── infer\n",
    "    │   ├── Airedale\n",
    "    │   │   ├── n10001.jpeg\n",
    "    │   │   └── n10002.jpeg\n",
    "    │   │   └── ...\n",
    "    │   │   ...\n",
    "    │   ├── basenji\n",
    "    │   │   ├── n12701.jpg\n",
    "    │   │   └── n12702.jpg\n",
    "    │   │   └── ...\n",
    "    │   │   ...\n",
    "    │  \n",
    "    ├── train\n",
    "    │   ├── Airedale\n",
    "    │   │   └── ...\n",
    "    │   │   ...\n",
    "    │   ├── basenji\n",
    "    │   │   └── ...\n",
    "    │   │   ...\n",
    "    │  \n",
    "    └── val\n",
    "        ├── Airedale\n",
    "        │   └── ...\n",
    "        │   ...\n",
    "        ├── basenji\n",
    "        │   └── ...\n",
    "        │   ...\n",
    "\n",
    "```\n",
    "\n",
    "以下为数据集划分的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73003d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "# list_name用来存储130种狗的种类名\n",
    "list_name = []\n",
    "# 数据集的存储路径\n",
    "load_path = 'D:/download/low-resolution/low-resolution'\n",
    "names = os.listdir(load_path)\n",
    "names = names[0:-1]\n",
    "# 路径中的文件名称为  x-xx-xxx，xxx就是狗的种类名\n",
    "names.sort(key=lambda x: int(x.split('-')[0]))\n",
    "for name in names:\n",
    "    list_name.append(name.split('-')[2])\n",
    "\n",
    "# 数据集的存储路径\n",
    "s1 = 'D:/download/low-resolution/low-resolution'\n",
    "# 划分后数据集的存储路径\n",
    "s2 = 'D:/download/low-resolution/datasets2/'\n",
    "# train、val和infer的文件路径\n",
    "train = s2 + 'train'\n",
    "val = s2 + 'val'\n",
    "infer = s2 + 'infer'\n",
    "# 创建文件目录\n",
    "os.makedirs(train)\n",
    "os.makedirs(val)\n",
    "os.makedirs(infer)\n",
    "for i in range(130):\n",
    "    os.makedirs(train + '/' + list_name[i])\n",
    "    os.makedirs(val + '/' + list_name[i])\n",
    "    os.makedirs(infer + '/' + list_name[i])\n",
    "    Files = os.listdir(s1)\n",
    "    path = s1 + '/' + Files[i]\n",
    "    files = os.listdir(path)\n",
    "    for j in tqdm.tqdm(range(len(files))):\n",
    "        # 按照8：1：1划分\n",
    "        old_file_path = path + '/' + files[j]\n",
    "        if j < len(files) * 0.8:\n",
    "            new_file_path = train + '/' + list_name[i] + '/' + files[j]\n",
    "        elif j < len(files) * 0.9:\n",
    "            new_file_path = val + '/' + list_name[i] + '/' + files[j]\n",
    "        else:\n",
    "            new_file_path = infer + '/' + list_name[i] + '/' + files[j]\n",
    "        # 从指定文件复制图片到另一指定文件\n",
    "        shutil.copy(old_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afd022",
   "metadata": {},
   "source": [
    "经过处理后的数据集的保存路径如下所示：\n",
    "\n",
    "```text\n",
    "──训练集：./datasets/train\n",
    "\n",
    "──验证集：./datasets/val\n",
    "\n",
    "──推理集:../datasets/infer\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d941c97",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "\n",
    "定义 `listdir`函数获取狗的种类名索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ab6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir(path, list_name):\n",
    "    i = 0\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            list_name[file] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec86b1",
   "metadata": {},
   "source": [
    "定义 `create_dataset`函数加载狗和牛角包数据集，对数据集进行图像增强操作并设置数据集batch_size大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be56dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as transforms\n",
    "\n",
    "def create_dataset(path, batch_size=20, train=True, image_size=224):\n",
    "\n",
    "    list_name = {}\n",
    "    train_path = \"/home/user2/LiDexin/datasets/train\"\n",
    "    listdir(train_path, list_name)\n",
    "    print(list_name)\n",
    "    dataset = ds.ImageFolderDataset(path, num_parallel_workers=8, class_indexing=list_name)\n",
    "\n",
    "    # 图像增强操作\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "    if train:\n",
    "        trans = [\n",
    "            transforms.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "            transforms.RandomHorizontalFlip(prob=0.5),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "            transforms.HWC2CHW()\n",
    "        ]\n",
    "    else:\n",
    "        trans = [\n",
    "            transforms.Decode(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "            transforms.HWC2CHW()\n",
    "        ]\n",
    "\n",
    "    dataset = dataset.map(operations=trans, input_columns=\"image\", num_parallel_workers=8)\n",
    "    # 设置batch_size的大小，若最后一次抓取的样本数小于batch_size，则丢弃\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8d117",
   "metadata": {},
   "source": [
    "加载训练数据集和验证数据集用于后续的模型训练和验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa19ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'affenpinscher': 0, 'Afghan_hound': 1, 'African_hunting_dog': 2, 'Airedale': 3, 'American_Staffordshire_terrier': 4, 'Appenzeller': 5, 'Australian_Shepherd': 6, 'Australian_terrier': 7, 'basenji': 8, 'basset': 9, 'beagle': 10, 'Bedlington_terrier': 11, 'Bernese_mountain_dog': 12, 'Bichon_Frise': 13, 'black_and_tan_coonhound': 14, 'Black_sable': 15, 'Blenheim_spaniel': 16, 'bloodhound': 17, 'bluetick': 18, 'Border_collie': 19, 'Border_terrier': 20, 'borzoi': 21, 'Boston_bull': 22, 'Bouvier_des_Flandres': 23, 'boxer': 24, 'Brabancon_griffo': 25, 'briard': 26, 'Brittany_spaniel': 27, 'bull_mastiff': 28, 'cairn': 29, 'Cane_Carso': 30, 'Cardigan': 31, 'Chesapeake_Bay_retriever': 32, 'Chihuahua': 33, 'Chinese_Crested_Dog': 34, 'chinese_rural_dog': 35, 'chow': 36, 'clumber': 37, 'cocker_spaniel': 38, 'collie': 39, 'curly_coated_retriever': 40, 'Dandie_Dinmont': 41, 'dhole': 42, 'dingo': 43, 'Doberman': 44, 'English_foxhound': 45, 'English_setter': 46, 'English_springer': 47, 'EntleBucher': 48, 'Eskimo_dog': 49, 'Fila Braziliero': 50, 'flat_coated_retriever': 51, 'French_bulldog': 52, 'German_shepherd': 53, 'German_short_haired_pointer': 54, 'giant_schnauzer': 55, 'golden_retriever': 56, 'Gordon_setter': 57, 'Great_Dane': 58, 'Great_Pyrenees': 59, 'Greater_Swiss_Mountain_dog': 60, 'groenendael': 61, 'Ibizan_hound': 62, 'Irish_setter': 63, 'Irish_terrier': 64, 'Irish_water_spaniel': 65, 'Irish_wolfhound': 66, 'Italian_greyhound': 67, 'Japanese_spaniel': 68, 'Japanese_Spitzes': 69, 'keeshond': 70, 'kelpie': 71, 'Kerry_blue_terrier': 72, 'komondor': 73, 'kuvasz': 74, 'Labrador_retriever': 75, 'Lakeland_terrier': 76, 'Leonberg': 77, 'Lhasa': 78, 'malamute': 79, 'malinois': 80, 'Maltese_dog': 81, 'Mexican_hairless': 82, 'miniature_pinscher': 83, 'miniature_poodle': 84, 'miniature_schnauzer': 85, 'Newfoundland': 86, 'Norfolk_terrier': 87, 'Norwegian_elkhound': 88, 'Norwich_terrier': 89, 'Old_English_sheepdog': 90, 'otterhound': 91, 'papillon': 92, 'Pekinese': 93, 'Pembroke': 94, 'Pomeranian': 95, 'pug': 96, 'redbone': 97, 'Rhodesian_ridgeback': 98, 'Rottweiler': 99, 'Saint_Bernard': 100, 'Saluki': 101, 'Samoyed': 102, 'schipperke': 103, 'Scotch_terrier': 104, 'Scottish_deerhound': 105, 'Sealyham_terrier': 106, 'Shetland_sheepdog': 107, 'Shiba_Dog': 108, 'Shih_Tzu': 109, 'Siberian_husky': 110, 'silky_terrier': 111, 'soft_coated_wheaten_terrier': 112, 'Staffordshire_bullterrier': 113, 'standard_poodle': 114, 'standard_schnauzer': 115, 'Sussex_spaniel': 116, 'teddy': 117, 'Tibetan_mastiff': 118, 'Tibetan_terrier': 119, 'toy_poodle': 120, 'toy_terrier': 121, 'vizsla': 122, 'Walker_hound': 123, 'Weimaraner': 124, 'Welsh_springer_spaniel': 125, 'West_Highland_white_terrier': 126, 'whippet': 127, 'wire_haired_fox_terrier': 128, 'Yorkshire_terrier': 129}\n",
      "{'affenpinscher': 0, 'Afghan_hound': 1, 'African_hunting_dog': 2, 'Airedale': 3, 'American_Staffordshire_terrier': 4, 'Appenzeller': 5, 'Australian_Shepherd': 6, 'Australian_terrier': 7, 'basenji': 8, 'basset': 9, 'beagle': 10, 'Bedlington_terrier': 11, 'Bernese_mountain_dog': 12, 'Bichon_Frise': 13, 'black_and_tan_coonhound': 14, 'Black_sable': 15, 'Blenheim_spaniel': 16, 'bloodhound': 17, 'bluetick': 18, 'Border_collie': 19, 'Border_terrier': 20, 'borzoi': 21, 'Boston_bull': 22, 'Bouvier_des_Flandres': 23, 'boxer': 24, 'Brabancon_griffo': 25, 'briard': 26, 'Brittany_spaniel': 27, 'bull_mastiff': 28, 'cairn': 29, 'Cane_Carso': 30, 'Cardigan': 31, 'Chesapeake_Bay_retriever': 32, 'Chihuahua': 33, 'Chinese_Crested_Dog': 34, 'chinese_rural_dog': 35, 'chow': 36, 'clumber': 37, 'cocker_spaniel': 38, 'collie': 39, 'curly_coated_retriever': 40, 'Dandie_Dinmont': 41, 'dhole': 42, 'dingo': 43, 'Doberman': 44, 'English_foxhound': 45, 'English_setter': 46, 'English_springer': 47, 'EntleBucher': 48, 'Eskimo_dog': 49, 'Fila Braziliero': 50, 'flat_coated_retriever': 51, 'French_bulldog': 52, 'German_shepherd': 53, 'German_short_haired_pointer': 54, 'giant_schnauzer': 55, 'golden_retriever': 56, 'Gordon_setter': 57, 'Great_Dane': 58, 'Great_Pyrenees': 59, 'Greater_Swiss_Mountain_dog': 60, 'groenendael': 61, 'Ibizan_hound': 62, 'Irish_setter': 63, 'Irish_terrier': 64, 'Irish_water_spaniel': 65, 'Irish_wolfhound': 66, 'Italian_greyhound': 67, 'Japanese_spaniel': 68, 'Japanese_Spitzes': 69, 'keeshond': 70, 'kelpie': 71, 'Kerry_blue_terrier': 72, 'komondor': 73, 'kuvasz': 74, 'Labrador_retriever': 75, 'Lakeland_terrier': 76, 'Leonberg': 77, 'Lhasa': 78, 'malamute': 79, 'malinois': 80, 'Maltese_dog': 81, 'Mexican_hairless': 82, 'miniature_pinscher': 83, 'miniature_poodle': 84, 'miniature_schnauzer': 85, 'Newfoundland': 86, 'Norfolk_terrier': 87, 'Norwegian_elkhound': 88, 'Norwich_terrier': 89, 'Old_English_sheepdog': 90, 'otterhound': 91, 'papillon': 92, 'Pekinese': 93, 'Pembroke': 94, 'Pomeranian': 95, 'pug': 96, 'redbone': 97, 'Rhodesian_ridgeback': 98, 'Rottweiler': 99, 'Saint_Bernard': 100, 'Saluki': 101, 'Samoyed': 102, 'schipperke': 103, 'Scotch_terrier': 104, 'Scottish_deerhound': 105, 'Sealyham_terrier': 106, 'Shetland_sheepdog': 107, 'Shiba_Dog': 108, 'Shih_Tzu': 109, 'Siberian_husky': 110, 'silky_terrier': 111, 'soft_coated_wheaten_terrier': 112, 'Staffordshire_bullterrier': 113, 'standard_poodle': 114, 'standard_schnauzer': 115, 'Sussex_spaniel': 116, 'teddy': 117, 'Tibetan_mastiff': 118, 'Tibetan_terrier': 119, 'toy_poodle': 120, 'toy_terrier': 121, 'vizsla': 122, 'Walker_hound': 123, 'Weimaraner': 124, 'Welsh_springer_spaniel': 125, 'West_Highland_white_terrier': 126, 'whippet': 127, 'wire_haired_fox_terrier': 128, 'Yorkshire_terrier': 129}\n"
     ]
    }
   ],
   "source": [
    "# 加载训练数据集\n",
    "train_path = \"/home/user2/LiDexin/datasets/train\"\n",
    "dataset_train = create_dataset(train_path, train=True)\n",
    "\n",
    "# 加载验证数据集\n",
    "val_path = \"/home/user2/LiDexin/datasets/val\"\n",
    "dataset_val = create_dataset(val_path, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546717f",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "本案例将采用MobileNetV2,ResNet50,ResNet101三个模型对数据集进行训练或微调。通过测试集测试后，我们从三个模型中选取效果最好的模型部署到手机。\n",
    "\n",
    "### MobileNet V2模型原理\n",
    "\n",
    "MobileNet网络是由Google团队于2017年提出的专注于移动端、嵌入式或IoT设备的轻量级CNN网络，相比于传统的卷积神经网络，MobileNet网络使用深度可分离卷积（Depthwise Separable Convolution）的思想在准确率小幅度降低的前提下，大大减小了模型参数与运算量。并引入宽度系数 $\\alpha$ 和分辨率系数 $\\beta$ 使模型满足不同应用场景的需求。\n",
    "\n",
    "由于MobileNet网络中Relu激活函数处理低维特征信息时会存在大量的丢失，所以MobileNetV2网络提出使用倒残差结构（Inverted residual block）和Linear Bottlenecks来设计网络，以提高模型的准确率，且优化后的模型更小。\n",
    "\n",
    "![mobilenet](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/tutorials/source_zh_cn/beginner/images/mobilenet.png)\n",
    "\n",
    "图中Inverted residual block结构是先使用1x1卷积进行升维，然后使用3x3的DepthWise卷积，最后使用1x1的卷积进行降维，与Residual block结构相反。Residual block是先使用1x1的卷积进行降维，然后使用3x3的卷积，最后使用1x1的卷积进行升维。\n",
    "\n",
    "> [详细内容可参见MobileNet V2论文](https://arxiv.org/pdf/1801.04381.pdf)\n",
    "\n",
    "### ResNet50模型原理\n",
    "\n",
    "ResNet在Pytorch官方代码中有5种不同深度的结构，分别为18、34、50、101、152（各网络深度指的是“需要通过训练更新参数“的层数，如卷积层，全连接层等），和论文完全一致。\n",
    "\n",
    "根据Block类型，可以将这五种ResNet分为两类：\n",
    "1）基于BasicBlock，浅层网络18，34都是由BasicBlock 搭成；\n",
    "2）基于Bottleneck，深层网络50，101，152是由Bottlen搭建而成；\n",
    "Block相当于积木，每个layer都由Block搭建构成，再由layer组成整个网络。每中ResNet都是4个layer（不算一开始的7*7卷积层和maxpooling层）。\n",
    "\n",
    "![resnet结构表.png](./image/resnet结构表.png)\n",
    "\n",
    "Resnet50 网络中包含了 49 个卷积层、1个全连接层。如下图所示，Resnet50网络结构可以分成七个部分，第一部分不包含残差块，主要对输入进行卷积、正则化、激活函数、最大池化的计算。第二、三、四、五部分结构都包含了残差块，图中的绿色图块不会改变残差块的尺寸，只用于改变残差块的维度。在 Resnet50 网 络 结 构 中 ， 残 差 块 都 有 三 层 卷 积 ， 那 网 络 总 共 有1+3×（3+4+6+3）=49个卷积层，加上最后的全连接层总共是 50 层，这也是Resnet50 名称的由来。网络的输入为 224×224×3，经过前五部分的卷积计算，输出为 7×7×2048，池化层会将其转化成一个特征向量，最后分类器会对这个特征向量进行计算并输出类别概率。\n",
    "\n",
    "![resnet50结构图.png](./image/resnet50结构图.png)\n",
    "\n",
    "### ResNet101模型原理\n",
    "\n",
    "上表是Resnet不同的结构，上表一共提出了5中深度的ResNet，分别是18，34，50，101和152，首先看表的最左侧，我们发现所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。 例如：101-layer那列，101-layer指的是101层网络，首先有个输入7x7x64的卷积，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1 + 99 + 1 = 101层，确实有101层网络； 注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；我们关注50-layer和101-layer这两列，可以发现，它们唯一的不同在于conv4_x，ResNet50有6个block，而ResNet101有23个block，两者之间差了17个block，也就是17 x 3 = 51层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6cceb",
   "metadata": {},
   "source": [
    "### 下载预训练模型\n",
    "\n",
    "#### MobileNet V2模型\n",
    "\n",
    "下载案例所需的[MobileNetV2预训练模型的ckpt文件](https://download.mindspore.cn/vision/classification/mobilenet_v2_1.0_224.ckpt)，预训练模型的宽度系数$\\alpha= 1.0$，输入图像大小为(224, 224), 将下载的预训练模型保存在当前目录下。使用MindSpore Vision中的`DownLoad`下载预训练模型文件到当前目录下，示例代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7669950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.dataset import DownLoad\n",
    "\n",
    "models_url1 = \"https://download.mindspore.cn/vision/classification/mobilenet_v2_1.0_224.ckpt\"\n",
    "\n",
    "dl1 = DownLoad()\n",
    "# 下载预训练模型文件\n",
    "dl1.download_url(models_url1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04197ef",
   "metadata": {},
   "source": [
    "#### ResNet50模型\n",
    "\n",
    "下载案例所需的[ResNet50预训练模型的ckpt文件](https://download.mindspore.cn/vision/classification/mobilenet_v2_1.0_224.ckpt)，输入图像大小为(224, 224), 将下载的预训练模型保存在当前目录下。使用MindSpore Vision中的`DownLoad`下载预训练模型文件到当前目录下，示例代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de395166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.dataset import DownLoad\n",
    "\n",
    "models_url = \"https://download.mindspore.cn/vision/classification/resnet50_224.ckpt\"\n",
    "\n",
    "dl = DownLoad()\n",
    "# 下载预训练模型文件\n",
    "dl.download_url(models_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe3f35",
   "metadata": {},
   "source": [
    "#### ResNet101模型\n",
    "\n",
    "下载案例所需的[ResNet101预训练模型的ckpt文件](https://download.mindspore.cn/vision/classification/resnet101_224.ckpt)，输入图像大小为(224, 224), 将下载的预训练模型保存在当前目录下。使用MindSpore Vision中的`DownLoad`下载预训练模型文件到当前目录下，示例代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.dataset import DownLoad\n",
    "\n",
    "models_url2 = \"https://download.mindspore.cn/vision/classification/mobilenet_v2_1.0_224.ckpt\"\n",
    "\n",
    "dl2 = DownLoad()\n",
    "# 下载预训练模型文件\n",
    "dl2.download_url(models_url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99a137",
   "metadata": {},
   "source": [
    "## MobileNet V2、Resnet50模型微调\n",
    "\n",
    "### 一、MobileNet V2模型微调\n",
    "\n",
    "本章使用MobileNet V2的预训练模型进行微调，通过删除MobileNet V2预训练模型中最后一个用于分类的1x1的卷积层的参数，使用全新数据集对模型进行重新训练以更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.train import Model\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "from mindvision.engine.loss import CrossEntropySmooth\n",
    "from typing import Any\n",
    "\n",
    "from mindvision.classification.models.backbones import MobileNetV2\n",
    "from mindvision.classification.models.classifiers import BaseClassifier\n",
    "from mindvision.classification.models.neck import GlobalAvgPooling\n",
    "from mindvision.classification.models.utils import make_divisible\n",
    "from mindvision.classification.utils.model_urls import model_urls\n",
    "from mindvision.utils.load_pretrained_model import LoadPretrainedModel\n",
    "\n",
    "__all__ = ['mobilenet_v2']\n",
    "\n",
    "def mobilenet_v2(num_classes: int = 1001,\n",
    "                 alpha: float = 1.0,\n",
    "                 round_nearest: int = 8,\n",
    "                 pretrained: bool = False,\n",
    "                 resize: int = 224,\n",
    "                 **kwargs: Any) -> MobileNetV2:\n",
    "    \"\"\"Mobilenet v2 structure.\"\"\"\n",
    "    backbone = MobileNetV2(alpha=alpha, round_nearest=round_nearest, **kwargs)\n",
    "    neck = GlobalAvgPooling(keep_dims=True)\n",
    "    inp_channel = make_divisible(1280 * max(1.0, alpha), round_nearest)\n",
    "    # head = ConvHead(input_channel=inp_channel, num_classes=num_classes)\n",
    "    head = CosFace(in_features=inp_channel, out_features=num_classes)\n",
    "    model = BaseClassifier(backbone, neck, head)\n",
    "\n",
    "    if pretrained:\n",
    "        # Download the pre-trained checkpoint file from url, and load\n",
    "        # checkpoint file.\n",
    "        arch = \"mobilenet_v2_\" + str(alpha) + \"_\" + str(resize)\n",
    "        LoadPretrainedModel(model, model_urls[arch]).run()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 创建模型,其中目标分类数为2，图像输入大小为(224,224)\n",
    "network = mobilenet_v2(num_classes=130, resize=224)\n",
    "\n",
    "# 定义优化器\n",
    "# network_opt = nn.Adam(params=network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "network_opt = nn.Adam(params=network.trainable_params())\n",
    "\n",
    "# 定义损失函数\n",
    "network_loss = CrossEntropySmooth(sparse=True, reduction=\"mean\", smooth_factor=0.1, classes_num=130)\n",
    "\n",
    "# 定义评价指标\n",
    "metrics = {\"Accuracy\": nn.Accuracy()}\n",
    "\n",
    "# 初始化模型\n",
    "model = Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2897a",
   "metadata": {},
   "source": [
    "> 上述WARNING是由于模型微调需要删除预训练模型的最后一个卷积层的参数，所以加载预训练模型会显示`head.classifier`参数未加载，`head.classifier`参数会使用创建模型时的初始化值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002055b",
   "metadata": {},
   "source": [
    "### 二、ResNet50模型微调\n",
    "\n",
    "同样的，此处采用Resnet50的预训练模型进行微调，也通过删除MobileNet V2预训练模型中最后一个用于分类的1x1的卷积层的参数，使用全新数据集对模型进行重新训练以更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.train import Model\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "from mindvision.classification.models import mobilenet_v2, resnet50\n",
    "from mindvision.engine.loss import CrossEntropySmooth\n",
    "\n",
    "# 创建模型,其中目标分类数为2，图像输入大小为(224,224)\n",
    "# network = mobilenet_v2(num_classes=38, resize=224)\n",
    "network = resnet50(130)\n",
    "\n",
    "# 模型参数存入到param_dict\n",
    "# param_dict = load_checkpoint(\"./mobilenet_v2_1.0_224.ckpt\")\n",
    "param_dict = load_checkpoint(\"./resnet50_224.ckpt\")\n",
    "\n",
    "# 获取mobilenet_v2网络最后一个卷积层的参数名\n",
    "filter_list = [x.name for x in network.head.get_parameters()]\n",
    "\n",
    "# 删除预训练模型的最后一个卷积层\n",
    "def filter_ckpt_parameter(origin_dict, param_filter):\n",
    "    for key in list(origin_dict.keys()):\n",
    "        for name in param_filter:\n",
    "            if name in key:\n",
    "                print(\"Delete parameter from checkpoint: \", key)\n",
    "                del origin_dict[key]\n",
    "                break\n",
    "\n",
    "filter_ckpt_parameter(param_dict, filter_list)\n",
    "\n",
    "# 加载预训练模型参数作为网络初始化权重\n",
    "load_param_into_net(network, param_dict)\n",
    "\n",
    "# 定义优化器\n",
    "network_opt = nn.Momentum(params=network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# 定义损失函数\n",
    "network_loss = CrossEntropySmooth(sparse=True, reduction=\"mean\", smooth_factor=0.1, classes_num=130)\n",
    "\n",
    "# 定义评价指标\n",
    "metrics = {\"Accuracy\": nn.Accuracy()}\n",
    "\n",
    "# 初始化模型\n",
    "model = Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70afb8",
   "metadata": {},
   "source": [
    "## Resnet101不使用预训练模型全新训练（修改优化器为Adam）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.classification.models import resnet101\n",
    "\n",
    "network = resnet101(num_classes=130, pretrained=False)\n",
    "\n",
    "# 定义优化器\n",
    "network_opt = nn.Adam(params=network.trainable_params())\n",
    "\n",
    "# 定义损失函数\n",
    "network_loss = CrossEntropySmooth(sparse=True, reduction=\"mean\", smooth_factor=0.1, classes_num=130)\n",
    "\n",
    "# 定义评价指标\n",
    "metrics = {\"Accuracy\": nn.Accuracy()}\n",
    "\n",
    "# 初始化模型\n",
    "model = Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13f4f4",
   "metadata": {},
   "source": [
    "### 模型训练与评估\n",
    "\n",
    "训练并评估网络，使用MindSpore Vision中的`mindvision.engine.callback.ValAccMonitor`接口打印训练的损失值和评估精度，且在训练完成后，保存评估精度最高的CKPT文件`best.ckpt`在当前目录下。\n",
    "\n",
    "### 此处由于训练时间过长，之后我们也将用训练好的本地ckpt文件来进行测试。\n",
    "\n",
    "resnet50:best_resnet50.ckpt\n",
    "\n",
    "resnet101:best_resnet101.ckpt\n",
    "\n",
    "mobilenetv2:best_mobilenetv2.skpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ce50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.engine.callback import ValAccMonitor\n",
    "from mindspore.train.callback import TimeMonitor\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# 模型训练与验证，训练完成后保存验证精度最高的ckpt文件（best.ckpt）到当前目录下\n",
    "model.train(num_epochs,\n",
    "            dataset_train,\n",
    "            callbacks=[ValAccMonitor(model, dataset_val, num_epochs), TimeMonitor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49ebd9",
   "metadata": {},
   "source": [
    "### 一、ResNet50----模型推理\n",
    "\n",
    "定义 `visualize_infer` 函数，我们选择用`Australian_Shepherd`狗种类数据集来进行预测，得到模型训练的准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415c16b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n103218.jpg', 'n103220.jpg', 'n103221.jpg', 'n103223.jpg', 'n103225.jpg', 'n103226.jpg', 'n103229.jpg', 'n103230.jpg', 'n103235.jpg', 'n103236.jpg', 'n103238.jpg', 'n103240.jpg', 'n103241.jpg', 'n103242.jpg', 'n103243.jpg', 'n103244.jpg', 'n103245.jpg', 'n103246.jpg', 'n103247.jpg', 'n103248.jpg', 'n103249.jpg', 'n103250.jpg', 'n103251.jpg', 'n103252.jpg', 'n1032522.jpg', 'n103253.jpg', 'n103254.jpg', 'n103255.jpg', 'n103256.jpg', 'n103258.jpg', 'n103259.jpg', 'n103260.jpg', 'n103261.jpg', 'n103263.jpg', 'n103264.jpg', 'n103265.jpg', 'n103266.jpg', 'n103267.jpg', 'n103268.jpg', 'n103271.jpg', 'n103272.jpg', 'n103273.jpg', 'n103274.jpg', 'n103275.jpg', 'n103276.jpg', 'n103277.jpg', 'n103278.jpg', 'n103280.jpg', 'n103281.jpg', 'n103282.jpg', 'n103283.jpg', 'n103284.jpg', 'n103286.jpg', 'n103287.jpg', 'n103288.jpg', 'n103289.jpg', 'n103291.jpg', 'n103292.jpg', 'n103293.jpg', 'n103294.jpg', 'n103296.jpg', 'n103297.jpg', 'n103301.jpg', 'n103302.jpg', 'n103303.jpg', 'n103307.jpg', 'n103309.jpg', 'n103310.jpg', 'n103311.jpg', 'n103312.jpg', 'n103313.jpg', 'n103315.jpg', 'n103317.jpg', 'n103320.jpg', 'n103321.jpg', 'n103322.jpg', 'n103323.jpg', 'n103324.jpg', 'n103325.jpg', 'n103326.jpg', 'n103327.jpg', 'n103329.jpg', 'n103330.jpg', 'n103331.jpg', 'n103332.jpg', 'n103333.jpg', 'n103334.jpg', 'n103335.jpg', 'n103337.jpg', 'n103338.jpg']\n",
      "\n",
      "\n",
      "\n",
      " resnet50_accuracy : 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mindvision.classification.models import resnet50\n",
    "from mindspore import Tensor\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore.train import Model\n",
    "\n",
    "def listdir2(path, list_name):\n",
    "    i = 0\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            list_name[i] = file\n",
    "            i += 1\n",
    "\n",
    "def visualize_infer(path):\n",
    "    path_list = os.listdir(path)\n",
    "    print(path_list)\n",
    "    average_result = 0\n",
    "    for i in range(len(path_list)):\n",
    "        path1 = path + \"//\" + path_list[i]\n",
    "        image = Image.open(path1).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "\n",
    "        # 归一化处理\n",
    "        mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "        std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "        image = np.array(image)\n",
    "        image = (image - mean) / std\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        # 图像通道由(h, w, c)转换为(c, h, w)\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # 扩展数据维数为(1, c, h, w)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # 定义并加载网络\n",
    "        net = resnet50(130)\n",
    "        param_dict = load_checkpoint(\"/home/user2/LiDexin/best_resnet50.ckpt\")\n",
    "        load_param_into_net(net, param_dict)\n",
    "        model = Model(net)\n",
    "\n",
    "        # 模型预测\n",
    "        pre = model.predict(Tensor(image))\n",
    "        result = np.argmax(pre)\n",
    "        list_name2 = {}\n",
    "        train_path = \"/home/user2/LiDexin/datasets/train\"\n",
    "        listdir2(train_path, list_name2)\n",
    "        class_name = list_name2\n",
    "        if class_name[result] == 'Australian_Shepherd':\n",
    "            average_result = average_result + 1\n",
    "    average_result = average_result / len(path_list)\n",
    "    return average_result\n",
    "\n",
    "path = \"/home/user2/LiDexin/datasets/infer/affenpinscher\"\n",
    "accuracy = visualize_infer(path).__str__()\n",
    "print('\\n\\n\\n resnet50_accuracy : ' + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf15b5c",
   "metadata": {},
   "source": [
    "### 二、MobileNetv2----模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceca551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n103218.jpg', 'n103220.jpg', 'n103221.jpg', 'n103223.jpg', 'n103225.jpg', 'n103226.jpg', 'n103229.jpg', 'n103230.jpg', 'n103235.jpg', 'n103236.jpg', 'n103238.jpg', 'n103240.jpg', 'n103241.jpg', 'n103242.jpg', 'n103243.jpg', 'n103244.jpg', 'n103245.jpg', 'n103246.jpg', 'n103247.jpg', 'n103248.jpg', 'n103249.jpg', 'n103250.jpg', 'n103251.jpg', 'n103252.jpg', 'n1032522.jpg', 'n103253.jpg', 'n103254.jpg', 'n103255.jpg', 'n103256.jpg', 'n103258.jpg', 'n103259.jpg', 'n103260.jpg', 'n103261.jpg', 'n103263.jpg', 'n103264.jpg', 'n103265.jpg', 'n103266.jpg', 'n103267.jpg', 'n103268.jpg', 'n103271.jpg', 'n103272.jpg', 'n103273.jpg', 'n103274.jpg', 'n103275.jpg', 'n103276.jpg', 'n103277.jpg', 'n103278.jpg', 'n103280.jpg', 'n103281.jpg', 'n103282.jpg', 'n103283.jpg', 'n103284.jpg', 'n103286.jpg', 'n103287.jpg', 'n103288.jpg', 'n103289.jpg', 'n103291.jpg', 'n103292.jpg', 'n103293.jpg', 'n103294.jpg', 'n103296.jpg', 'n103297.jpg', 'n103301.jpg', 'n103302.jpg', 'n103303.jpg', 'n103307.jpg', 'n103309.jpg', 'n103310.jpg', 'n103311.jpg', 'n103312.jpg', 'n103313.jpg', 'n103315.jpg', 'n103317.jpg', 'n103320.jpg', 'n103321.jpg', 'n103322.jpg', 'n103323.jpg', 'n103324.jpg', 'n103325.jpg', 'n103326.jpg', 'n103327.jpg', 'n103329.jpg', 'n103330.jpg', 'n103331.jpg', 'n103332.jpg', 'n103333.jpg', 'n103334.jpg', 'n103335.jpg', 'n103337.jpg', 'n103338.jpg']\n",
      "\n",
      "\n",
      "\n",
      " mobilenetv2_accuracy : 0.7888888888888889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mindvision.classification.models import mobilenet_v2\n",
    "from mindspore import Tensor\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore.train import Model\n",
    "\n",
    "def listdir2(path, list_name):\n",
    "    i = 0\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            list_name[i] = file\n",
    "            i += 1\n",
    "\n",
    "def visualize_infer(path):\n",
    "    path_list = os.listdir(path)\n",
    "    print(path_list)\n",
    "    average_result = 0\n",
    "    for i in range(len(path_list)):\n",
    "        path1 = path + \"//\" + path_list[i]\n",
    "        image = Image.open(path1).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "\n",
    "        # 归一化处理\n",
    "        mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "        std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "        image = np.array(image)\n",
    "        image = (image - mean) / std\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        # 图像通道由(h, w, c)转换为(c, h, w)\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # 扩展数据维数为(1, c, h, w)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # 定义并加载网络\n",
    "        net = mobilenet_v2(num_classes=130, resize=224)\n",
    "        param_dict = load_checkpoint(\"/home/user2/LiDexin/best_mobilenetv2.ckpt\")\n",
    "        load_param_into_net(net, param_dict)\n",
    "        model = Model(net)\n",
    "\n",
    "        # 模型预测\n",
    "        pre = model.predict(Tensor(image))\n",
    "        result = np.argmax(pre)\n",
    "        list_name2 = {}\n",
    "        train_path = \"/home/user2/LiDexin/datasets/train\"\n",
    "        listdir2(train_path, list_name2)\n",
    "        class_name = list_name2\n",
    "        if class_name[result] == 'Australian_Shepherd':\n",
    "            average_result = average_result + 1\n",
    "    average_result = average_result / len(path_list)\n",
    "    return average_result\n",
    "\n",
    "path = \"/home/user2/LiDexin/datasets/infer/affenpinscher\"\n",
    "accuracy = visualize_infer(path).__str__()\n",
    "print('\\n\\n\\n mobilenetv2_accuracy : ' + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff08949",
   "metadata": {},
   "source": [
    "### 三、ResNet101----模型推理（此网络是尝试全新训练，未加载预训练模型直接通过数据集训练得到的模型）\n",
    "\n",
    "#### 由于时间问题，且此模型较大，我们只训练了50个epoch,效果达到了如下图的效果：其验证精度值大概在70%\n",
    "\n",
    "![resnet101结果图.png](./image/resnet101结果图.png)\n",
    "\n",
    "#### 且由于ResNet101的模型过于复杂，参数量较大，即使是下载了ckpt文件在自己的服务器进行测试还是暴显存，且可能时间有限，其达到的精度也不是很理想，\n",
    "\n",
    "#### 所以我们后续部署中也就没有采用resnet101的模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5873c0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n103218.jpg', 'n103220.jpg', 'n103221.jpg', 'n103223.jpg', 'n103225.jpg', 'n103226.jpg', 'n103229.jpg', 'n103230.jpg', 'n103235.jpg', 'n103236.jpg', 'n103238.jpg', 'n103240.jpg', 'n103241.jpg', 'n103242.jpg', 'n103243.jpg', 'n103244.jpg', 'n103245.jpg', 'n103246.jpg', 'n103247.jpg', 'n103248.jpg', 'n103249.jpg', 'n103250.jpg', 'n103251.jpg', 'n103252.jpg', 'n1032522.jpg', 'n103253.jpg', 'n103254.jpg', 'n103255.jpg', 'n103256.jpg', 'n103258.jpg', 'n103259.jpg', 'n103260.jpg', 'n103261.jpg', 'n103263.jpg', 'n103264.jpg', 'n103265.jpg', 'n103266.jpg', 'n103267.jpg', 'n103268.jpg', 'n103271.jpg', 'n103272.jpg', 'n103273.jpg', 'n103274.jpg', 'n103275.jpg', 'n103276.jpg', 'n103277.jpg', 'n103278.jpg', 'n103280.jpg', 'n103281.jpg', 'n103282.jpg', 'n103283.jpg', 'n103284.jpg', 'n103286.jpg', 'n103287.jpg', 'n103288.jpg', 'n103289.jpg', 'n103291.jpg', 'n103292.jpg', 'n103293.jpg', 'n103294.jpg', 'n103296.jpg', 'n103297.jpg', 'n103301.jpg', 'n103302.jpg', 'n103303.jpg', 'n103307.jpg', 'n103309.jpg', 'n103310.jpg', 'n103311.jpg', 'n103312.jpg', 'n103313.jpg', 'n103315.jpg', 'n103317.jpg', 'n103320.jpg', 'n103321.jpg', 'n103322.jpg', 'n103323.jpg', 'n103324.jpg', 'n103325.jpg', 'n103326.jpg', 'n103327.jpg', 'n103329.jpg', 'n103330.jpg', 'n103331.jpg', 'n103332.jpg', 'n103333.jpg', 'n103334.jpg', 'n103335.jpg', 'n103337.jpg', 'n103338.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.867.981 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.868.009 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.874.009 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.876.326 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.876.340 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.879.913 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.880.896 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.880.913 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.884.333 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.885.301 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.885.317 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.888.757 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.889.711 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.889.727 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.893.196 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.894.149 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.894.163 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.897.649 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.898.605 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.898.620 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.902.138 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.903.096 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.903.110 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.906.622 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.907.579 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.907.594 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.911.118 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.912.075 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.912.089 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.915.610 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.916.578 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.916.593 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.920.112 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.921.082 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.921.096 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.924.622 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.925.581 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.925.597 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.929.145 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.930.102 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.930.117 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.933.636 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.934.595 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.934.611 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.938.168 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.939.131 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.939.146 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.942.689 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.943.655 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[9437184].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.943.671 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.947.243 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.948.215 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[4194304].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.948.230 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.951.768 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.952.742 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[8388608].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.952.758 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.956.303 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.957.298 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[4194304].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.957.313 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.960.860 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.961.817 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[9437184].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.961.832 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.965.387 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.966.349 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[4194304].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.966.365 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.969.914 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.970.876 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[4194304].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.970.892 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.974.455 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.975.416 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[9437184].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.975.431 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.978.987 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.979.940 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:249] CalMemBlockAllocSize] Memory not enough: current free memory size[4063232] is smaller than required size[4194304].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.979.957 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.983.513 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.984.487 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.984.502 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f343a7fc700,python):2022-07-03-23:24:53.988.047 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f3439ffb700,python):2022-07-03-23:24:53.989.591 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:255] CalMemBlockAllocSize] Device memory size [4063232] is smaller than minimum alloc size [10485760].\n",
      "[WARNING] PRE_ACT(24147,7f3439ffb700,python):2022-07-03-23:24:53.989.609 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:525] DumpDynamicMemPoolDebugInfo] Start dump dynamic memory pool debug info.\n",
      "[WARNING] PRE_ACT(24147,7f3439ffb700,python):2022-07-03-23:24:53.993.193 [mindspore/ccsrc/common/mem_reuse/mem_dynamic_allocator.cc:528] DumpDynamicMemPoolDebugInfo] Finish dump dynamic memory pool debug info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CRITICAL] RUNTIME_FRAMEWORK(24147,7f34f7cc52c0,python):2022-07-03-23:24:55.876.336 [mindspore/ccsrc/runtime/graph_scheduler/graph_scheduler.cc:619] Run] Device(id:0) memory isn't enough and alloc failed, kernel name: Default/backbone-ResNet/conv1-ConvNormActivation/features-SequentialCell/0-Conv2d/Conv2D-op117047, alloc size: 3211264B.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mindspore/ccsrc/runtime/graph_scheduler/graph_scheduler.cc:619 Run] Device(id:0) memory isn't enough and alloc failed, kernel name: Default/backbone-ResNet/conv1-ConvNormActivation/features-SequentialCell/0-Conv2d/Conv2D-op117047, alloc size: 3211264B.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_24147/3473027144.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/home/user2/LiDexin/datasets/infer/affenpinscher\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m \u001B[0maccuracy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvisualize_infer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__str__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n\\n\\n mobilenetv2_accuracy : '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0maccuracy\u001B[0m \u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_24147/3473027144.py\u001B[0m in \u001B[0;36mvisualize_infer\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;31m# 模型预测\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m         \u001B[0mpre\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     47\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpre\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0mlist_name2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/train/model.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, *predict_data)\u001B[0m\n\u001B[1;32m   1114\u001B[0m         \u001B[0mcheck_input_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mpredict_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_class\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1115\u001B[0m         \u001B[0m_parallel_predict_check\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1116\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_predict_network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mpredict_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0mcheck_output_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/nn/cell.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    584\u001B[0m                 logger.warning(f\"For 'Cell', it's not support hook function in graph mode. If you want to use hook \"\n\u001B[1;32m    585\u001B[0m                                f\"function, please use context.set_context to set pynative mode.\")\n\u001B[0;32m--> 586\u001B[0;31m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile_and_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/nn/cell.py\u001B[0m in \u001B[0;36mcompile_and_run\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m    987\u001B[0m                 \u001B[0mparallel_inputs_run\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew_inputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    988\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0m_cell_graph_executor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mparallel_inputs_run\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mphase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mphase\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 989\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_cell_graph_executor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mnew_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mphase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mphase\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    990\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    991\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mauto_parallel_compile_and_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/common/api.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, obj, phase, *args)\u001B[0m\n\u001B[1;32m   1083\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"precompile_only\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_is_role_pserver\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_is_role_sched\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1085\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mphase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mphase\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1086\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1087\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0m_wrap_func\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/common/api.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, obj, phase, *args)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         \u001B[0mphase_real\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mphase\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_time\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marguments_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1109\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhas_compiled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mphase_real\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_exec_pip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mphase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mphase_real\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'{} graph is not exist.'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mphase_real\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/common/api.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*arg, **kwargs)\u001B[0m\n\u001B[1;32m     88\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_convert_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/common/api.py\u001B[0m in \u001B[0;36m_exec_pip\u001B[0;34m(self, obj, phase, *args)\u001B[0m\n\u001B[1;32m   1090\u001B[0m         \u001B[0mfn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1091\u001B[0m         \u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__parse_method__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1092\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_graph_executor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mphase\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1093\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1094\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mphase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'predict'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mindspore/ccsrc/runtime/graph_scheduler/graph_scheduler.cc:619 Run] Device(id:0) memory isn't enough and alloc failed, kernel name: Default/backbone-ResNet/conv1-ConvNormActivation/features-SequentialCell/0-Conv2d/Conv2D-op117047, alloc size: 3211264B."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mindvision.classification.models import resnet101\n",
    "from mindspore import Tensor\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore.train import Model\n",
    "\n",
    "def listdir2(path, list_name):\n",
    "    i = 0\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            list_name[i] = file\n",
    "            i += 1\n",
    "\n",
    "def visualize_infer(path):\n",
    "    path_list = os.listdir(path)\n",
    "    print(path_list)\n",
    "    average_result = 0\n",
    "    for i in range(len(path_list)):\n",
    "        path1 = path + \"//\" + path_list[i]\n",
    "        image = Image.open(path1).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "\n",
    "        # 归一化处理\n",
    "        mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "        std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "        image = np.array(image)\n",
    "        image = (image - mean) / std\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        # 图像通道由(h, w, c)转换为(c, h, w)\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # 扩展数据维数为(1, c, h, w)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # 定义并加载网络\n",
    "        net = resnet101(num_classes=130)\n",
    "        param_dict = load_checkpoint(\"/home/user2/LiDexin/best_resnet101.ckpt\")\n",
    "        load_param_into_net(net, param_dict)\n",
    "        model = Model(net)\n",
    "\n",
    "        # 模型预测\n",
    "        pre = model.predict(Tensor(image))\n",
    "        result = np.argmax(pre)\n",
    "        list_name2 = {}\n",
    "        train_path = \"/home/user2/LiDexin/datasets/train\"\n",
    "        listdir2(train_path, list_name2)\n",
    "        class_name = list_name2\n",
    "        if class_name[result] == 'Australian_Shepherd':\n",
    "            average_result = average_result + 1\n",
    "    average_result = average_result / len(path_list)\n",
    "    return average_result\n",
    "\n",
    "path = \"/home/user2/LiDexin/datasets/infer/affenpinscher\"\n",
    "accuracy = visualize_infer(path).__str__()\n",
    "print('\\n\\n\\n mobilenetv2_accuracy : ' + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48dab2",
   "metadata": {},
   "source": [
    "### 模型导出\n",
    "\n",
    "在模型训练完后，我们综合三个模型训练时的验证精度和测试集的推理精度综合，选取综合最优的网络模型（即CKPT文件）转换为MindIR格式，用于后续手机侧的推理。通过`export`接口会在当前目录下会生成`model_v2_1.0_224.mindir`文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd913fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from mindspore import export, Tensor\n",
    "\n",
    "# 定义并加载网络参数\n",
    "net = mobilenet_v2(num_classes=130, resize=224)\n",
    "#param_dict = load_checkpoint(\"best.ckpt\")\n",
    "#param_dict = load_checkpoint(\"best.ckpt\")\n",
    "param_dict = load_checkpoint(\"/home/user2/LiDexin/best_mobilenetv2.ckpt\")\n",
    "load_param_into_net(net, param_dict)\n",
    "\n",
    "# 将模型由ckpt格式导出为MINDIR格式\n",
    "input_np = np.random.uniform(0.0, 1.0, size=[1, 3, 224, 224]).astype(np.float32)\n",
    "export(net, Tensor(input_np), file_name=\"model_v2_1.0_224\", file_format=\"MINDIR\")\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c41f63",
   "metadata": {},
   "source": [
    "## 手机侧推理与部署\n",
    "\n",
    "为实现模型文件在手机侧的推理功能，步骤如下：\n",
    "\n",
    "- 转换文件格式：将MindIR文件格式，转换成Android手机上MindSpore Lite可识别文件；\n",
    "- 应用部署：在手机侧部署应用APK，即下载一个MindSpore Vision套件Android APK；\n",
    "- 应用体验：最后将ms模型文件导入到手机侧后，体验狗的分类识别功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f5ac0",
   "metadata": {},
   "source": [
    "### 应用体验\n",
    "\n",
    "将上述训练的自定义网络模型mobilenet_v2_1.0_224.ms部署到Android手机侧，体验狗的分类识别功能。\n",
    "\n",
    "#### 自定义模型标签文件\n",
    "\n",
    "自定义模型部署需要按照如下格式定义网络模型需要用到的信息，即自定义标签文件，并在本地电脑端侧创建一个必须以`custom.json`命名的json格式标签文件。\n",
    "\n",
    "```text\n",
    "{\n",
    "    \"title\": '狗品种分类',\n",
    "    \"file\": 'model_v2_1.0_224.ms',\n",
    "    \"label\": ['Airedale', 'miniature_poodle', 'affenpinscher', 'schipperke', 'Australian_terrier', 'Welsh_springer_spaniel', 'curly_coated_retriever', 'Staffordshire_bullterrier', 'Norwich_terrier', 'Tibetan_terrier', 'English_setter', 'Norfolk_terrier', 'Pembroke', 'Tibetan_mastiff', 'Border_terrier', 'Great_Dane', 'Scotch_terrier', 'flat_coated_retriever', 'Saluki', 'Irish_setter', 'Blenheim_spaniel', 'Irish_terrier', 'bloodhound', 'redbone', 'West_Highland_white_terrier', 'Brabancon_griffo', 'dhole', 'kelpie', 'Doberman', 'Ibizan_hound', 'vizsla', 'cairn', 'German_shepherd', 'African_hunting_dog', 'Dandie_Dinmont', 'Sealyham_terrier', 'German_short_haired_pointer', 'Bernese_mountain_dog', 'Saint_Bernard', 'Leonberg', 'Bedlington_terrier', 'Newfoundland', 'Lhasa', 'Chesapeake_Bay_retriever', 'Lakeland_terrier', 'Walker_hound', 'American_Staffordshire_terrier', 'otterhound', 'Sussex_spaniel', 'Norwegian_elkhound', 'bluetick', 'dingo', 'Irish_water_spaniel', 'Fila Braziliero', 'standard_schnauzer', 'Mexican_hairless', 'EntleBucher', 'Afghan_hound', 'kuvasz', 'English_foxhound', 'keeshond', 'Irish_wolfhound', 'Scottish_deerhound', 'Rottweiler', 'black_and_tan_coonhound', 'Great_Pyrenees', 'boxer', 'wire_haired_fox_terrier', 'borzoi', 'groenendael', 'collie', 'Gordon_setter', 'Kerry_blue_terrier', 'briard', 'Rhodesian_ridgeback', 'Boston_bull', 'bull_mastiff', 'silky_terrier', 'Brittany_spaniel', 'Eskimo_dog', 'giant_schnauzer', 'malinois', 'Bouvier_des_Flandres', 'whippet', 'Appenzeller', 'Chinese_Crested_Dog', 'soft_coated_wheaten_terrier', 'Weimaraner', 'clumber', 'Greater_Swiss_Mountain_dog', 'toy_terrier', 'Italian_greyhound', 'basset', 'basenji', 'Australian_Shepherd', 'Maltese_dog', 'Japanese_spaniel', 'Cane_Carso', 'Japanese_Spitzes', 'Old_English_sheepdog', 'Black_sable', 'Shetland_sheepdog', 'English_springer', 'beagle', 'cocker_spaniel', 'standard_poodle', 'komondor', 'chow', 'Yorkshire_terrier', 'Shih_Tzu', 'Chihuahua', 'Pekinese', 'miniature_pinscher', 'pug', 'papillon', 'Shiba_Dog', 'French_bulldog', 'Siberian_hus]\n",
    "}\n",
    "```\n",
    "\n",
    "Json标签文件中需包含`title`，`file`，`label`三个Key值字段，其含义如下：\n",
    "\n",
    "- title ：自定义模块标题(狗品种分类)；\n",
    "- file ：上文转换好的模型文件名称；\n",
    "- label ：自定义标签label的`数组`信息。\n",
    "\n",
    "#### 标签与模型文件部署到手机\n",
    "\n",
    "在`MindSpore Vision APK`的首页上长按`分类`按钮，可以进入自定义分类模式，并且选择需要部署的标签和模型文件。\n",
    "\n",
    "为实现手机端狗的分类识别功能，需将标签文件`custom.json`文件和模型文件`model_v2_1.0_224.ms`一起放置到手机上指定目录下。这里以`Android/data/Download/` 文件夹为例，首先把标签文件和模型文件同时放在上述手机地址，如图所示，点击自定义按钮，然后会弹出系统文件功能，点击左上角的打开文件，然后找到Json标签文件和模型文件存放的目录地址，并选择对应的Json文件。\n",
    "\n",
    "标签与模型文件部署到手机后，即可点击中间按钮进行拍照获取图片，或者点击上侧栏的图像按钮选择图片相册用于图像，就可以进行狗品种分类识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d9b12",
   "metadata": {},
   "source": [
    "## 实际效果\n",
    "\n",
    "### 但是由于分类的数目过多，导致物体识别的置信度很低，但是准确率还是能达到比较好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc5857",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![手机1.png](./image/手机1.jpg)\n",
    "![手机2.png](./image/手机2.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}